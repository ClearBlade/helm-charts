{{- if eq (default .Values.global.gmpEnabled false) false}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: {{ default "monitoring" .Values.global.namespace }}
data:
  prometheus.rules: |-
   groups:
   - name: Alerts
     rules:
      {{- if .Values.alerts.containerRestart }}
     - alert: ContainerRestart
       expr: time() - container_start_time_seconds{container=~"(clearblade|cb-.*)",container!~".*exporter",namespace="{{ $.Values.global.deploymentnamespace }}"} < 300
       for: 1m
       annotations:
         summary: "{{ "{{" }} $labels.container }} container on node {{ "{{" }} $labels.namespace }} restarted"
      {{- end}}
      {{- if .Values.alerts.highCPU }}
     - alert: HighCPU
       expr: round(sum(rate(container_cpu_usage_seconds_total{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="long-running-test"}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="long-running-test"}/container_spec_cpu_period{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="long-running-test"}) by (pod, container)) > 80
       for: 10m
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} cpu is too high: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if .Values.alerts.fileDescriptors }}
     - alert: TooManyOpenFileDescriptors
       expr: node_filefd_allocated * on(instance) group_left(nodename) (node_uname_info) > 900000
       for: 2m
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} has too many open file descriptors: {{ "{{" }} $value }}/999999"
      {{- end}}
      {{- if .Values.alerts.disk }}
     - alert: DiskUsageDB
       expr: round(100.0 * kubelet_volume_stats_used_bytes{persistentvolumeclaim="cb-postgres-1"} / kubelet_volume_stats_capacity_bytes) > 1
       for: 2m
       annotations:
         summary: "DB disk is getting pretty full on {{ "{{" }} $labels.namespace }}: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if .Values.alerts.highMem }}
     - alert: HighMemoryUsage
       expr: round(sum(container_memory_usage_bytes{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="long-running-test"}) by (pod, container) / sum(container_spec_memory_limit_bytes{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="long-running-test"}) by (pod, container)) > 90
       for: 2m
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} memory usage is too high: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if .Values.alerts.LRS }}
     - alert: DroppingMessagesInLRS
       expr: rate(clearblade_stream_services_dropped_messages{service_name!="all_services"}[5m]) > 0
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} is dropping messages"
      {{- end}}
    {{- if .Values.alerts.expensiveStuff }}
   - name: ExpensiveStuff
     rules:
     - record: http_succeeded:5m_increase
       expr: sum by(instance) (increase(clearblade_http_succeeded[5m]))
     - record: http_succeeded:1h_by_endpoint
       expr: sum by(method,endpoint) (increase(clearblade_http_succeeded[1h]))
     - record: http_failed:5m_increase
       expr: sum by(instance) (increase(clearblade_http_failed[5m]))
     - record: http_failed:1h_by_endpoint
       expr: sum by(method,endpoint) (increase(clearblade_http_failed[1h]))
     - record: mqtt_publishes:5m_avg
       expr: sum by(instance) (rate(clearblade_mqtt_publishes[5m])) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
     - record: timers_succeeded:5m_count
       expr: sum by(system_key, timer_name, service_name) (increase(clearblade_timers_successful[5m]))
     - record: timers_failed:5m_count
       expr: sum by (system_key, timer_name, service_name) (increase(clearblade_timers_failed[5m]))
     - record: triggers_succeeded:5m_count
       expr: sum by(system_key, trigger_name) (increase(clearblade_triggers_successful[5m]))
     - record: triggers_failed:5m_count
       expr: sum by(system_key, trigger_name) (increase(clearblade_triggers_failed[5m]))
     - record: mqtt_connections
       expr: sum by(nodename, connection_type)(clearblade_mqtt_current_connections{connection_type!="all_connections"} * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info)))
     - record: mqtt_dropped:5m_rate
       expr: rate(clearblade_stream_services_dropped_messages{service_name!="all_services"}[5m]) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
     - record: mqtt_dropped:total
       expr: sum(clearblade_stream_services_dropped_messages{service_name!="all_services"}) by(service_name,topic_path)
     - record: services_running:by_name
       expr: sum by(system_key,service_name) (clearblade_code_services_running_services{system_key!="all_systems"})
     - record: services_running:by_node
       expr: sum by(instance) (clearblade_code_services_running_services{system_key!="all_systems"}) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
    {{- end}}
  prometheus.yml: |-
{{ (tpl (.Files.Get "configs/prometheus.yml") .) | indent 4 }}
{{- end}}
