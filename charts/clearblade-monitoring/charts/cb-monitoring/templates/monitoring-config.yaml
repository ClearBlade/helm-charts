{{- if eq (default .Values.global.gmpEnabled false) false}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: {{ default "monitoring" .Values.global.namespace }}
data:
  prometheus.rules: |-
   groups:
   - name: Alerts
     rules:
      {{- range $n := .Values.global.deploymentNamespaces }}
      {{- if $.Values.alerts.containerRestartEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} Container Restart
       expr: increase(kube_pod_container_status_restarts_total{namespace="{{ $n.name }}", container=~"cb-.*|clearblade"}[5m]) > 0
       for: 1m
       labels:
         priority: {{ $.Values.alerts.containerRestartPriority }}
         severity: warning
       annotations:
         summary: "{{ "{{" }} $labels.container }} container on {{ "{{" }} $labels.namespace }} restarted"
      {{- end}}
      {{- if $.Values.alerts.podNotReadyEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} PodNotReady
       expr: kube_pod_container_status_ready{job="kube-state-metrics",namespace="{{ $n.name }}", container=~"cb-.*|clearblade"} == 0
       for: 15m
       labels:
         priority: {{ $.Values.alerts.podNotReadPriority }}
         severity: warning
       annotations:
         summary: Pod {{ "{{" }} $labels.pod }} container {{ "{{" }} $labels.container }} is not ready
         description: >
           Container {{ "{{" }} $labels.container }} in Pod {{ "{{" }} $labels.pod }} (Namespace {{ "{{" }} $labels.namespace }})
           has been in a not-ready state for more than 15 minutes.
      {{- end}}
      {{- if $.Values.alerts.podFailedEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} PodFailed
       expr: kube_pod_status_phase{job="kube-state-metrics",phase="Failed",namespace="{{ $n.name }}"} == 1
       for: 1m
       labels:
         priority: {{ $.Values.alerts.podFailedPriority }}
         severity: warning
       annotations:
         summary: Pod {{ "{{" }} $labels.pod }} has failed
         description: >
           Pod {{ "{{" }} $labels.pod }} (Namespace {{ "{{" }} $labels.namespace }}) is in a Failed state.
      {{- end}}
      {{- if $.Values.alerts.podOOMEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} PodOOMKilled
       expr: increase(kube_pod_container_status_last_terminated_reason{job="kube-state-metrics",reason="OOMKilled",namespace="{{ $n.name }}"}[5m]) > 0
       for: 1m
       labels:
         priority: {{ $.Values.alerts.podOOMPriority }}
         severity: warning
       annotations:
         summary: Pod {{ "{{" }} $labels.pod }} container {{ "{{" }} $labels.container }} terminated due to OOM
         description: >
           Container {{ "{{" }} $labels.container }} in Pod {{ "{{" }} $labels.pod }} (Namespace {{ "{{" }} $labels.namespace }})
           was terminated with OOMKilled reason.
      {{- end}}
      {{- if $.Values.alerts.highCPUEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} HighCPU
       expr: round(100.0 * sum(rate(container_cpu_usage_seconds_total{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="{{ $n.name }}"}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="{{ $n.name }}"}/container_spec_cpu_period{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="{{ $n.name }}"}) by (pod, container)) > {{ $.Values.alerts.highCPUThreshold }}
       for: 10m
       labels:
         priority: {{ $.Values.alerts.highCPUPriority }}
         severity: warning
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} cpu is too high: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if $.Values.alerts.fileDescriptorsEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} TooManyOpenFileDescriptors
       expr: container_file_descriptors{container=~"(clearblade|cb-.*)",namespace="{{ $n.name }}"} > {{ $.Values.alerts.fileDescriptorsThreshold }}
       for: 2m
       labels:
         priority: {{ $.Values.alerts.fileDescriptorsPriority }}
         severity: warning
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} has too many open file descriptors: {{ "{{" }} $value }}/999999"
      {{- end}}
      {{- if $.Values.alerts.diskUsageEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} DiskUsageDB
       expr: round(100.0 * kubelet_volume_stats_used_bytes{namespace="{{ $n.name }}"} / kubelet_volume_stats_capacity_bytes) > {{ $.Values.alerts.diskUsageThreshold }}
       for: 2m
       labels:
         priority: {{ $.Values.alerts.diskUsagePriority }}
         severity: warning
       annotations:
         summary: "DB disk is getting pretty full on {{ "{{" }} $labels.namespace }}: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if $.Values.alerts.highMemEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} HighMemoryUsage
       expr: round(100.0 * sum(container_memory_usage_bytes{container=~"(clearblade|cb-.*)",container!~".*exporter", container!="cb-postgres", namespace="{{ $n.name }}"}) by (pod, container) / sum(container_spec_memory_limit_bytes{container=~"(clearblade|cb-.*)",container!~".*exporter", namespace="{{ $n.name }}"}) by (pod, container)) > {{ $.Values.alerts.highMemThreshold }}
       for: 2m
       labels:
         priority: {{ $.Values.alerts.highMemPriority }}
         severity: warning
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} memory usage is too high: {{ "{{" }} $value }}%"
      {{- end}}
      {{- if $.Values.alerts.LRSDropsEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} DroppingMessagesInLRS
       expr: rate(clearblade_stream_services_dropped_messages{service_name!="all_services",namespace="{{ $n.name }}"}[5m]) > {{ $.Values.alerts.LRSDropsThreshold }}
       labels:
         priority: {{ $.Values.alerts.LRSDropsPriority }}
         severity: warning
       annotations:
         summary: "Pod {{ "{{" }} $labels.pod }} is dropping messages"
      {{- end}}
      {{- if $.Values.alerts.adminConnectionsEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} HighDatabaseConnections
       expr: pg_stat_database_numbackends{datname="admin",job=~".*{{ $n.name }}"} > {{ $.Values.alerts.adminConnectionsThreshold }}
       for: 5m
       labels:
         priority: {{ $.Values.alerts.adminConnectionsPriority }}
         severity: warning
       annotations:
         summary: "High number of database connections to admin database"
         description: "The number of connections to the admin database has exceeded the threshold of {{ $.Values.alerts.adminConnectionsThreshold }}. "
      {{- end}}
      {{- if $.Values.alerts.HTTPFailureEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} HighHttpFailureRate
       expr: sum(rate(clearblade_http_failed{namespace="{{ $n.name }}"}[5m])) > {{ $.Values.alerts.HTTPFailureThreshold }}
       for: 5m
       labels:
         priority: {{ $.Values.alerts.HTTPFailurePriority }}
         severity: warning
       annotations:
         summary: "High HTTP failure rate"
         description: "The HTTP failure rate is above {{ $.Values.alerts.HTTPFailureThreshold }}/second for past 5 minutes"
      {{- end}}
      {{- if $.Values.alerts.HTTPFailureIncreaseEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} IncreasingHttpFailures
       expr: ( (increase(clearblade_http_failed{namespace="{{ $n.name }}"}[10m]) - clearblade_http_failed{namespace="{{ $n.name }}"} offset 10m) / (clearblade_http_failed{namespace="{{ $n.name }}"} offset 10m) ) > ({{ $.Values.alerts.HTTPFailureIncreaseThreshold }} / 100)
       labels:
         priority: {{ $.Values.alerts.HTTPFailureIncreasePriority }}
         severity: warning
       annotations:
         summary: "Rapidly increasing HTTP failures"
         description: "The number of HTTP failures is increasing rapidly"
      {{- end}}
      {{- if $.Values.alerts.lowMQTTConnectionsEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} LowMqttConnections
       expr: sum(clearblade_mqtt_current_connections{namespace="{{ $n.name }}"}) < {{ $.Values.alerts.lowMQTTConnectionsThreshold }}
       for: 5m
       labels:
         priority: {{ $.Values.alerts.lowMQTTConnectionsPriority }}
         severity: warning
       annotations:
         summary: "Low MQTT connections"
         description: "The number of MQTT connections is below {{ $.Values.alerts.lowMQTTConnectionsThreshold }}"
      {{- end}}
      {{- if $.Values.alerts.MQTTDisconnectDerivEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} RapidMqttConnectionLoss
       expr: sum(-deriv(clearblade_mqtt_current_connections{namespace="{{ $n.name }}"}[10m])) > {{ $.Values.alerts.MQTTDisconnectDerivThreshold }}
       labels:
         priority: {{ $.Values.alerts.MQTTDisconnectDerivPriority }}
         severity: warning
       annotations:
         summary: "Rapid loss of MQTT connections"
         description: "The number of MQTT connections is dropping rapidly."
      {{- end }}
      {{- if $.Values.alerts.licenseExpiryEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} LicenseExpiring
       expr: clearblade_expiration{instance="clearblade-monitoring-service-0.{{ $n.name }}.svc.cluster.local"} < {{ $.Values.alerts.licenseExpiryThreshold }}
       for: 1m
       labels:
         priority: {{ $.Values.alerts.licenseExpiryPriority }}
         frequency: daily
         severity: warning
       annotations:
         summary: "License expires in {{ "{{" }} $value }} days!"
      {{- end }}
      {{- if $.Values.alerts.possibleDBConnectionMaxEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} PossibleDBConnectionMaxOut
       expr: sum(clearblade_up{job=~".*{{ $n.name }}"}) * 30 > pg_settings_max_connections{job=~".*{{ $n.name }}"}
       labels:
         priority: {{ $.Values.alerts.possibleDBConnectionMaxPriority }}
         severity: warning
       annotations:
         summary: "Possible postgres connection max out"
         description: "The number of clearblade nodes running can potentially max out the postgres connection pool"
      {{- end }}
      {{- if $.Values.alerts.redisConnectionMaxEnabled }}
     - alert: {{ $.Values.global.customerName }} {{$n.name}} RedisConnectionsMaxOutWarning
       expr: (redis_connected_clients{job=~".*{{ $n.name }}"}/redis_max_clients{job=~".*{{ $n.name }}"})*100 > 80
       labels:
         priority: {{ $.Values.alerts.redisConnectionMaxPriority }}
         severity: warning
       annotations:
         summary: "The Redis connection pool is filling up"
         description: "The number of connections to redis is using 80% of the connection pool"
      {{- end }}
      {{- end }}
    {{- if $.Values.alerts.expensiveStuff }}
   - name: ExpensiveStuff
     rules:
     - record: http_succeeded:5m_increase
       expr: sum by(instance) (increase(clearblade_http_succeeded[5m]))
     - record: http_succeeded:1h_by_endpoint
       expr: sum by(method,endpoint) (increase(clearblade_http_succeeded[1h]))
     - record: http_failed:5m_increase
       expr: sum by(instance) (increase(clearblade_http_failed[5m]))
     - record: http_failed:1h_by_endpoint
       expr: sum by(method,endpoint) (increase(clearblade_http_failed[1h]))
     - record: mqtt_publishes:5m_avg
       expr: sum by(instance) (rate(clearblade_mqtt_publishes[5m])) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
     - record: timers_succeeded:5m_count
       expr: sum by(system_key, timer_name, service_name) (increase(clearblade_timers_successful[5m]))
     - record: timers_failed:5m_count
       expr: sum by (system_key, timer_name, service_name) (increase(clearblade_timers_failed[5m]))
     - record: triggers_succeeded:5m_count
       expr: sum by(system_key, trigger_name) (increase(clearblade_triggers_successful[5m]))
     - record: triggers_failed:5m_count
       expr: sum by(system_key, trigger_name) (increase(clearblade_triggers_failed[5m]))
     - record: mqtt_connections
       expr: sum by(nodename, connection_type)(clearblade_mqtt_current_connections{connection_type!="all_connections"} * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info)))
     - record: mqtt_dropped:5m_rate
       expr: rate(clearblade_stream_services_dropped_messages{service_name!="all_services"}[5m]) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
     - record: mqtt_dropped:total
       expr: sum(clearblade_stream_services_dropped_messages{service_name!="all_services"}) by(service_name,topic_path)
     - record: services_running:by_name
       expr: sum by(system_key,service_name) (clearblade_code_services_running_services{system_key!="all_systems"})
     - record: services_running:by_node
       expr: sum by(instance) (clearblade_code_services_running_services{system_key!="all_systems"}) * on(instance) group_left(nodename) (sum by(instance,nodename) (node_uname_info))
    {{- end}}
  prometheus.yml: |-
{{ (tpl (.Files.Get "configs/prometheus.yml") .) | indent 4 }}
{{- end}}
